{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_fruit_Prediction_s6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "waczF5VBtaH1"
      },
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import pandas as pd \n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, GRU, TimeDistributed, RepeatVector, Lambda, Bidirectional\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Sequential, load_model, clone_model\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from keras.backend import clear_session\n",
        "import time\n",
        "import csv\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec3D-KO4E9Bj"
      },
      "source": [
        "# 所有城市對照英文代碼\n",
        "city = {\n",
        "    '基隆市':'KLU',\n",
        "    '臺北市':'TPE',\n",
        "    '新北市':'TPH',\n",
        "    '桃園市':'TYC',\n",
        "    '新竹市':'HSC',\n",
        "    '新竹縣':'HSH',\n",
        "    '苗栗縣':'MAL',\n",
        "    '臺中市':'TXG',\n",
        "    '彰化縣':'CWH',\n",
        "    '南投縣':'NTO',\n",
        "    '雲林縣':'YLH',\n",
        "    '嘉義市':'CYI',\n",
        "    '嘉義縣':'CHY',\n",
        "    '臺南市':'TNN',\n",
        "    '高雄市':'KHH',\n",
        "    '屏東縣':'IUH',\n",
        "    '宜蘭縣':'ILN',\n",
        "    '花蓮縣':'HWA',\n",
        "    '臺東縣':'TTT'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGpB0wffQiiT"
      },
      "source": [
        "# 資料與模型參數確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jDX4kfLFRBr"
      },
      "source": [
        "# 農作物\n",
        "crop_dict = {\n",
        "    1 : ['cabbage', '高麗菜', '(LA1 甘藍 初秋)'],\n",
        "    2 : ['carrot', '胡蘿蔔', '(SB2 胡蘿蔔 清洗)'],\n",
        "    3 : ['beeftomato', '牛番茄', '(FJ3 番茄 牛蕃茄)'],\n",
        "    4 : ['cucumber', '胡瓜', '(FD1 花胡瓜)'],\n",
        "    5 : ['loofah', '絲瓜', '(FF1 絲瓜)'],\n",
        "    6 : ['cabbage2', '包心白菜', '(LC1 包心白 包白)'],\n",
        "    7 : ['shallots', '青蔥', '(SE6 青蔥 粉蔥)'],\n",
        "    8 : ['bittergourd', '苦瓜', '(FG1 苦瓜 白大米)'],\n",
        "    9 : ['onion', '洋蔥', '(SD1 洋蔥 本產)'],\n",
        "    10 : ['waterspinach', '空心菜', '(LF2 蕹菜 小葉)'],\n",
        "\n",
        "    11 : ['guava', '番石榴', '(P1 番石榴 珍珠芭)'],\n",
        "    12 : ['pineapple', '鳳梨', '(B2 鳳梨 金鑽鳳梨)'],\n",
        "    13 : ['papaya', '木瓜', '(I1 木瓜 網室紅肉)'],\n",
        "    14 : ['watermelon', '西瓜', '(T1 西瓜 大西瓜)'],\n",
        "    15 : ['banana', '香蕉', '(A1 香蕉)'],\n",
        "    16 : ['apple', '蘋果', '(X69 蘋果 富士進口)'],\n",
        "    17 : ['pear', '梨子', '(O4 梨 新興梨)'],\n",
        "    18 : ['grape', '葡萄', '(S1 葡萄 巨峰)'],\n",
        "    19 : ['dragonfruit', '火龍果', '(812 火龍果 紅肉)'],\n",
        "    20 : ['mango', '芒果', '(R1 芒果 愛文)'],\n",
        "    21 : ['pakchoy', '青江菜', '(LD1 青江白菜 小梗)'], # 此處與colab有變動\n",
        "    22 : ['cauliflower', '花椰菜', '(FB11 花椰菜 青梗 留梗炳)'],\n",
        "    23 : ['lemon', '檸檬', '(F1 雜柑 檸檬)'],\n",
        "    24 : ['tomato', '小番茄', '(74 小番茄 玉女)'], # 此處與colab有變動\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afvy4drzFSsn"
      },
      "source": [
        "# 市場選定\n",
        "market_dict = {\n",
        "    1 : '台北一'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_3bsRsyQVNy"
      },
      "source": [
        "1. 時間設定\n",
        "2. 資料區間\n",
        "3. 模型相關選擇\n",
        "4. 作圖\n",
        "5. 儲存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzYVyEoWQRQW",
        "outputId": "7135417e-1541-4a5b-b0d2-dd24feddaac5"
      },
      "source": [
        "# 1. \n",
        "# 取得現在時間(TP) 並設定為 +8 時區\n",
        "time_now = datetime.now(timezone(timedelta(hours=+8))).isoformat(timespec=\"seconds\")[5:16].replace('-', '', 1).replace('T', '-')\n",
        "\n",
        "# 2.\n",
        "# 資料區間\n",
        "# 選擇農產品，(13-24為水果)\n",
        "crop_no = 17\n",
        "dataset_path = '/content/drive/MyDrive/Ai團專_農時_共享資料夾/Dataset_資料集/模型訓練之資料集/fruit model vseion/model_dataset_'\n",
        "print(dataset_path + crop_dict[crop_no][1] + '.csv')\n",
        "# 選擇市場，目前只有1\n",
        "market_no = 1\n",
        "# 是否要刪除價格空值\n",
        "price_na_del = True\n",
        "ohe = True\n",
        "\n",
        "# 是否要加入天氣資料\n",
        "# add_weather_data = True\n",
        "add_weather_data = False\n",
        "# # 是否要加入颱風資料\n",
        "add_typhoon_data = True\n",
        "# add_typhoon_data = False\n",
        "# 訂定訓練資料的期間、測試資料的期間\n",
        "train_start_date = '2013-01-02'\n",
        "train_end_date = '2020-05-31'\n",
        "test_start_date = '2020-06-01'\n",
        "test_end_date = '2021-06-18'\n",
        "# 設定往前以及往後看的天數, 若many to many, past_day, future_day要設定一樣的值\n",
        "pastDay = 10\n",
        "futureDay = 7\n",
        "\n",
        "# 3.\n",
        "# 使用哪一個模型，目前有1-3\n",
        "model_no = 3\n",
        "repeat_train = True\n",
        "# 模型每層參數 數量\n",
        "LSTM_unit_1 = 10\n",
        "LSTM_unit_2 = ''\n",
        "# LSTM_unit_1 = 256\n",
        "# LSTM_unit_2 = 16\n",
        "# LSTM_unit_3 = 64\n",
        "# 其它參數\n",
        "batch_size = 30\n",
        "# epochs = 1000\n",
        "epochs = 500\n",
        "validation_split = 0.1\n",
        "patience = 50\n",
        "\n",
        "# 4.\n",
        "# 作圖\n",
        "# 畫多少天的預測圖，要小於或等於上面的數字\n",
        "plotDay = 1\n",
        "# 畫到數一共幾天的預測圖\n",
        "pic_days = 300\n",
        "\n",
        "# 5.\n",
        "# 是否要存入google drive\n",
        "save_google = True\n",
        "# save_route = '/content/drive/MyDrive/Ai團專_農時_共享資料夾/模型與成果/'\n",
        "save_google_dir = '/content/drive/MyDrive/AI_project/result/'\n",
        "dev_notes = ''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Ai團專_農時_共享資料夾/Dataset_資料集/模型訓練之資料集/fruit model vseion/model_dataset_梨子.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgVZL1KU5IVK"
      },
      "source": [
        "# 版本介紹與環境"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7pjLObL5SOd"
      },
      "source": [
        "價格預測版本s6，\n",
        "此版本為 最終版\n",
        "以市場為核心，選擇性與颱風氣候資料合併\n",
        "訓練過程自動抓取五次訓練中的最佳結果進行儲存\n",
        "\n",
        "重要資訊簡介:\n",
        "1. 氣候資料為?日 vs 1天價格\n",
        "2. 進行shift\n",
        "3. 2000.01.02 or 2013.01.02開始\n",
        "4. 所有參數數據及重要模型參數將自動儲存\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9inBQJikkt_",
        "outputId": "f43d1f80-7938-4256-c0b5-ea8cdace95ad"
      },
      "source": [
        "from google.colab import drive\n",
        "if save_google: drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "834O6MchZrU3"
      },
      "source": [
        "def download_data():\n",
        "    # 檔案下載url   \n",
        "    weather_data_url = 'https://github.com/Yi-Wei-Lin/Tibame_AI_Project/raw/main/userdata/amoswu/dataset/reportdaily_mean_fillna.csv'\n",
        "    typhoon_data_url = 'https://github.com/Yi-Wei-Lin/Tibame_AI_Project/raw/main/userdata/amoswu/dataset/TyphoonDatabase.csv'\n",
        "    price_data_url = 'https://github.com/Yi-Wei-Lin/Tibame_AI_Project/raw/main/userdata/lynnbai/dataset/Banana.csv'\n",
        "    # 將檔案下載至colab\n",
        "    if not os.path.exists('weather.csv'): urllib.request.urlretrieve(weather_data_url, 'weather.csv') \n",
        "    if not os.path.exists('typhoon.csv'): urllib.request.urlretrieve(typhoon_data_url, 'typhoon.csv') \n",
        "    if not os.path.exists('banana.csv'): urllib.request.urlretrieve(price_data_url, 'banana.csv')\n",
        "download_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhXEQmRT-YzY"
      },
      "source": [
        "# for i in city_drop_list:\n",
        "#     city_drop_columns.append([s for s in list(df_date.columns) if s.__contains__(i)])\n",
        "# city_drop_columns = list(np.array(city_drop_columns).reshape(-1))\n",
        "\n",
        "# dataset_path = '/content/drive/MyDrive/Ai團專_農時_共享資料夾/Dataset_資料集/模型訓練之資料集/fruit model vseion/model_dataset_木瓜.csv'\n",
        "# dataset_path = '/content/drive/MyDrive/Ai團專_農時_共享資料夾/Dataset_資料集/模型訓練之資料集/fruit model vseion/model_dataset_'\n",
        "# asdf = pd.read_csv(dataset_path + crop_dict[crop_no+1][1] + '.csv', encoding='utf-8')\n",
        "# asdf\n",
        "# crop_dict[13][1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj-IAZIwn0aV"
      },
      "source": [
        "# 資料預處理 - 天氣"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1L-GJXvJSrO"
      },
      "source": [
        "# 要移除的欄位列表\n",
        "# weather columb全部列表: 'date', 'city', 'StnPres', 'SeaPres', 'StnPresMax', 'StnPresMaxTime', 'StnPresMin', 'StnPresMinTime', 'Temperature', 'TMax', 'TMaxTime', 'TMin', 'TMinTime', 'TdDewPoint', 'RH', 'RHMin', 'RHMinTime', 'WS', 'WD', 'WSGust', 'WDGust', 'WGustTime', 'Precp', 'PrecpHour', 'PrecpMax10', 'PrecpMax10Time', 'PrecpMax60', 'PrecpMax60Time', 'SunShine', 'SunShineRate', 'GloblRad', 'VisbMean', 'EvapA', 'UVIMax', 'UVIMaxTime', 'CloudAmount'\n",
        "weather_drop_columns = [\n",
        "              'StnPres', 'SeaPres', 'StnPresMax', 'StnPresMaxTime', \n",
        "              'StnPresMin', 'StnPresMinTime', 'RHMin',  'WSGust', \n",
        "              'GloblRad', 'VisbMean', 'UVIMax', 'UVIMaxTime', 'CloudAmount'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEW9khK9JWfE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "7fc822a0-bc84-4eb8-d1cd-260e5a9c9187"
      },
      "source": [
        "df = pd.read_csv('weather.csv', encoding='utf-8')\n",
        "df = df.drop(weather_drop_columns, axis=1)\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>city</th>\n",
              "      <th>Temperature</th>\n",
              "      <th>TMax</th>\n",
              "      <th>TMaxTime</th>\n",
              "      <th>TMin</th>\n",
              "      <th>TMinTime</th>\n",
              "      <th>TdDewPoint</th>\n",
              "      <th>RH</th>\n",
              "      <th>RHMinTime</th>\n",
              "      <th>WS</th>\n",
              "      <th>WD</th>\n",
              "      <th>WDGust</th>\n",
              "      <th>WGustTime</th>\n",
              "      <th>Precp</th>\n",
              "      <th>PrecpHour</th>\n",
              "      <th>PrecpMax10</th>\n",
              "      <th>PrecpMax10Time</th>\n",
              "      <th>PrecpMax60</th>\n",
              "      <th>PrecpMax60Time</th>\n",
              "      <th>SunShine</th>\n",
              "      <th>SunShineRate</th>\n",
              "      <th>EvapA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000-01-01</td>\n",
              "      <td>南投縣</td>\n",
              "      <td>9.0</td>\n",
              "      <td>14.3</td>\n",
              "      <td>2000-01-01 13:53:00</td>\n",
              "      <td>6.5</td>\n",
              "      <td>2000-01-01 19:22:00</td>\n",
              "      <td>1.1</td>\n",
              "      <td>60.0</td>\n",
              "      <td>2000-01-01 12:46:00</td>\n",
              "      <td>7.5</td>\n",
              "      <td>301.0</td>\n",
              "      <td>262.5</td>\n",
              "      <td>2000-01-01 17:41:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.8</td>\n",
              "      <td>91.5</td>\n",
              "      <td>2.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000-01-01</td>\n",
              "      <td>嘉義市</td>\n",
              "      <td>18.9</td>\n",
              "      <td>26.0</td>\n",
              "      <td>2000-01-01 13:37:00</td>\n",
              "      <td>13.4</td>\n",
              "      <td>2000-01-01 23:44:00</td>\n",
              "      <td>14.3</td>\n",
              "      <td>77.0</td>\n",
              "      <td>2000-01-01 15:53:00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2000-01-01 11:57:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.6</td>\n",
              "      <td>71.2</td>\n",
              "      <td>2.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2000-01-01</td>\n",
              "      <td>嘉義縣</td>\n",
              "      <td>14.0</td>\n",
              "      <td>14.9</td>\n",
              "      <td>2000-01-01 11:45:00</td>\n",
              "      <td>3.7</td>\n",
              "      <td>2000-01-01 21:50:00</td>\n",
              "      <td>-3.6</td>\n",
              "      <td>44.0</td>\n",
              "      <td>2000-01-01 01:25:00</td>\n",
              "      <td>1.8</td>\n",
              "      <td>300.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>2000-01-01 06:18:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.2</td>\n",
              "      <td>96.5</td>\n",
              "      <td>2.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         date city  Temperature  ...  SunShine SunShineRate  EvapA\n",
              "0  2000-01-01  南投縣          9.0  ...       9.8         91.5    2.8\n",
              "1  2000-01-01  嘉義市         18.9  ...       7.6         71.2    2.8\n",
              "2  2000-01-01  嘉義縣         14.0  ...       9.2         96.5    2.1\n",
              "\n",
              "[3 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 993
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnbsB7J3muaU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "c25dca13-23f2-47c3-f209-2163e19523b2"
      },
      "source": [
        "# 使用index做merge，將df表格依日期拉平\n",
        "df_date = df['date'].drop_duplicates().to_frame().set_index('date')\n",
        "\n",
        "for cityname, citycode in city.items():\n",
        "    df_city = df.loc[df['city'] == cityname].add_suffix('_' + citycode).set_index('date' + '_' + citycode)\n",
        "    df_date = pd.merge(df_date, df_city, how='left', left_index = True, right_index = True)\n",
        "# 將城市名稱欄位移除\n",
        "df_date = df_date[df_date.columns.drop(list(df_date.filter(regex='city')))]\n",
        "df_weather = df_date\n",
        "df_weather.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Temperature_KLU</th>\n",
              "      <th>TMax_KLU</th>\n",
              "      <th>TMaxTime_KLU</th>\n",
              "      <th>TMin_KLU</th>\n",
              "      <th>TMinTime_KLU</th>\n",
              "      <th>TdDewPoint_KLU</th>\n",
              "      <th>RH_KLU</th>\n",
              "      <th>RHMinTime_KLU</th>\n",
              "      <th>WS_KLU</th>\n",
              "      <th>WD_KLU</th>\n",
              "      <th>WDGust_KLU</th>\n",
              "      <th>WGustTime_KLU</th>\n",
              "      <th>Precp_KLU</th>\n",
              "      <th>PrecpHour_KLU</th>\n",
              "      <th>PrecpMax10_KLU</th>\n",
              "      <th>PrecpMax10Time_KLU</th>\n",
              "      <th>PrecpMax60_KLU</th>\n",
              "      <th>PrecpMax60Time_KLU</th>\n",
              "      <th>SunShine_KLU</th>\n",
              "      <th>SunShineRate_KLU</th>\n",
              "      <th>EvapA_KLU</th>\n",
              "      <th>Temperature_TPE</th>\n",
              "      <th>TMax_TPE</th>\n",
              "      <th>TMaxTime_TPE</th>\n",
              "      <th>TMin_TPE</th>\n",
              "      <th>TMinTime_TPE</th>\n",
              "      <th>TdDewPoint_TPE</th>\n",
              "      <th>RH_TPE</th>\n",
              "      <th>RHMinTime_TPE</th>\n",
              "      <th>WS_TPE</th>\n",
              "      <th>WD_TPE</th>\n",
              "      <th>WDGust_TPE</th>\n",
              "      <th>WGustTime_TPE</th>\n",
              "      <th>Precp_TPE</th>\n",
              "      <th>PrecpHour_TPE</th>\n",
              "      <th>PrecpMax10_TPE</th>\n",
              "      <th>PrecpMax10Time_TPE</th>\n",
              "      <th>PrecpMax60_TPE</th>\n",
              "      <th>PrecpMax60Time_TPE</th>\n",
              "      <th>SunShine_TPE</th>\n",
              "      <th>...</th>\n",
              "      <th>TMaxTime_HWA</th>\n",
              "      <th>TMin_HWA</th>\n",
              "      <th>TMinTime_HWA</th>\n",
              "      <th>TdDewPoint_HWA</th>\n",
              "      <th>RH_HWA</th>\n",
              "      <th>RHMinTime_HWA</th>\n",
              "      <th>WS_HWA</th>\n",
              "      <th>WD_HWA</th>\n",
              "      <th>WDGust_HWA</th>\n",
              "      <th>WGustTime_HWA</th>\n",
              "      <th>Precp_HWA</th>\n",
              "      <th>PrecpHour_HWA</th>\n",
              "      <th>PrecpMax10_HWA</th>\n",
              "      <th>PrecpMax10Time_HWA</th>\n",
              "      <th>PrecpMax60_HWA</th>\n",
              "      <th>PrecpMax60Time_HWA</th>\n",
              "      <th>SunShine_HWA</th>\n",
              "      <th>SunShineRate_HWA</th>\n",
              "      <th>EvapA_HWA</th>\n",
              "      <th>Temperature_TTT</th>\n",
              "      <th>TMax_TTT</th>\n",
              "      <th>TMaxTime_TTT</th>\n",
              "      <th>TMin_TTT</th>\n",
              "      <th>TMinTime_TTT</th>\n",
              "      <th>TdDewPoint_TTT</th>\n",
              "      <th>RH_TTT</th>\n",
              "      <th>RHMinTime_TTT</th>\n",
              "      <th>WS_TTT</th>\n",
              "      <th>WD_TTT</th>\n",
              "      <th>WDGust_TTT</th>\n",
              "      <th>WGustTime_TTT</th>\n",
              "      <th>Precp_TTT</th>\n",
              "      <th>PrecpHour_TTT</th>\n",
              "      <th>PrecpMax10_TTT</th>\n",
              "      <th>PrecpMax10Time_TTT</th>\n",
              "      <th>PrecpMax60_TTT</th>\n",
              "      <th>PrecpMax60Time_TTT</th>\n",
              "      <th>SunShine_TTT</th>\n",
              "      <th>SunShineRate_TTT</th>\n",
              "      <th>EvapA_TTT</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2000-01-01</th>\n",
              "      <td>19.3</td>\n",
              "      <td>22.6</td>\n",
              "      <td>2000-01-01 12:28:30</td>\n",
              "      <td>16.9</td>\n",
              "      <td>2000-01-01 22:33:30</td>\n",
              "      <td>14.3</td>\n",
              "      <td>73.5</td>\n",
              "      <td>2000-01-01 19:56:00</td>\n",
              "      <td>3.8</td>\n",
              "      <td>280.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>2000-01-01 12:36:30</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.9</td>\n",
              "      <td>2000-01-01 03:00:00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>2000-01-01 02:57:00</td>\n",
              "      <td>3.8</td>\n",
              "      <td>36.6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>17.5</td>\n",
              "      <td>21.1</td>\n",
              "      <td>2000-01-01 14:05:44</td>\n",
              "      <td>14.3</td>\n",
              "      <td>2000-01-01 22:33:55</td>\n",
              "      <td>11.9</td>\n",
              "      <td>78.7</td>\n",
              "      <td>2000-01-01 19:43:00</td>\n",
              "      <td>1.2</td>\n",
              "      <td>230.9</td>\n",
              "      <td>197.3</td>\n",
              "      <td>2000-01-01 11:02:27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.7</td>\n",
              "      <td>...</td>\n",
              "      <td>2000-01-01 12:10:40</td>\n",
              "      <td>13.5</td>\n",
              "      <td>2000-01-01 02:05:50</td>\n",
              "      <td>16.4</td>\n",
              "      <td>74.0</td>\n",
              "      <td>2000-01-01 10:56:00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>221.7</td>\n",
              "      <td>162.7</td>\n",
              "      <td>2000-01-01 10:54:40</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.1</td>\n",
              "      <td>37.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>19.2</td>\n",
              "      <td>25.0</td>\n",
              "      <td>2000-01-01 11:34:18</td>\n",
              "      <td>17.7</td>\n",
              "      <td>2000-01-01 05:33:12</td>\n",
              "      <td>16.9</td>\n",
              "      <td>74.8</td>\n",
              "      <td>2000-01-01 08:28:30</td>\n",
              "      <td>1.8</td>\n",
              "      <td>119.2</td>\n",
              "      <td>75.2</td>\n",
              "      <td>2000-01-01 13:10:42</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2000-01-01 01:45:00</td>\n",
              "      <td>0.1</td>\n",
              "      <td>2000-01-01 01:45:00</td>\n",
              "      <td>3.1</td>\n",
              "      <td>28.9</td>\n",
              "      <td>2.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-01-02</th>\n",
              "      <td>18.6</td>\n",
              "      <td>21.7</td>\n",
              "      <td>2000-01-02 11:51:30</td>\n",
              "      <td>15.2</td>\n",
              "      <td>2000-01-02 15:26:00</td>\n",
              "      <td>11.4</td>\n",
              "      <td>64.5</td>\n",
              "      <td>2000-01-02 12:48:30</td>\n",
              "      <td>4.7</td>\n",
              "      <td>130.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>2000-01-02 20:07:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.5</td>\n",
              "      <td>79.8</td>\n",
              "      <td>2.6</td>\n",
              "      <td>16.8</td>\n",
              "      <td>22.6</td>\n",
              "      <td>2000-01-02 13:02:11</td>\n",
              "      <td>12.1</td>\n",
              "      <td>2000-01-02 08:10:22</td>\n",
              "      <td>9.0</td>\n",
              "      <td>68.7</td>\n",
              "      <td>2000-01-02 04:59:40</td>\n",
              "      <td>1.9</td>\n",
              "      <td>98.3</td>\n",
              "      <td>93.7</td>\n",
              "      <td>2000-01-02 20:01:05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.6</td>\n",
              "      <td>...</td>\n",
              "      <td>2000-01-02 11:30:51</td>\n",
              "      <td>14.6</td>\n",
              "      <td>2000-01-02 11:12:43</td>\n",
              "      <td>13.7</td>\n",
              "      <td>62.0</td>\n",
              "      <td>2000-01-02 11:45:00</td>\n",
              "      <td>1.1</td>\n",
              "      <td>215.7</td>\n",
              "      <td>139.9</td>\n",
              "      <td>2000-01-02 11:21:43</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.6</td>\n",
              "      <td>23.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>17.9</td>\n",
              "      <td>22.1</td>\n",
              "      <td>2000-01-02 11:34:11</td>\n",
              "      <td>16.5</td>\n",
              "      <td>2000-01-02 11:06:44</td>\n",
              "      <td>14.1</td>\n",
              "      <td>64.0</td>\n",
              "      <td>2000-01-02 13:42:15</td>\n",
              "      <td>2.8</td>\n",
              "      <td>119.8</td>\n",
              "      <td>89.7</td>\n",
              "      <td>2000-01-02 14:44:27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.2</td>\n",
              "      <td>38.9</td>\n",
              "      <td>3.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-01-03</th>\n",
              "      <td>18.7</td>\n",
              "      <td>20.8</td>\n",
              "      <td>2000-01-03 13:12:00</td>\n",
              "      <td>16.3</td>\n",
              "      <td>2000-01-03 13:59:30</td>\n",
              "      <td>10.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>2000-01-03 12:57:00</td>\n",
              "      <td>3.9</td>\n",
              "      <td>60.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>2000-01-03 03:55:30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>47.4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>17.6</td>\n",
              "      <td>20.8</td>\n",
              "      <td>2000-01-03 12:44:49</td>\n",
              "      <td>15.3</td>\n",
              "      <td>2000-01-03 05:40:11</td>\n",
              "      <td>10.7</td>\n",
              "      <td>76.7</td>\n",
              "      <td>2000-01-03 13:20:40</td>\n",
              "      <td>2.3</td>\n",
              "      <td>79.2</td>\n",
              "      <td>89.0</td>\n",
              "      <td>2000-01-03 10:31:44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.6</td>\n",
              "      <td>...</td>\n",
              "      <td>2000-01-03 11:25:43</td>\n",
              "      <td>12.9</td>\n",
              "      <td>2000-01-03 09:30:09</td>\n",
              "      <td>13.5</td>\n",
              "      <td>67.0</td>\n",
              "      <td>2000-01-03 13:55:00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>210.9</td>\n",
              "      <td>172.4</td>\n",
              "      <td>2000-01-03 13:15:09</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.8</td>\n",
              "      <td>16.7</td>\n",
              "      <td>2.5</td>\n",
              "      <td>16.9</td>\n",
              "      <td>22.2</td>\n",
              "      <td>2000-01-03 12:24:00</td>\n",
              "      <td>16.2</td>\n",
              "      <td>2000-01-03 09:16:12</td>\n",
              "      <td>13.5</td>\n",
              "      <td>64.2</td>\n",
              "      <td>2000-01-03 11:48:45</td>\n",
              "      <td>2.2</td>\n",
              "      <td>101.1</td>\n",
              "      <td>66.7</td>\n",
              "      <td>2000-01-03 05:31:42</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.2</td>\n",
              "      <td>47.8</td>\n",
              "      <td>3.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 399 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Temperature_KLU  TMax_KLU  ... SunShineRate_TTT  EvapA_TTT\n",
              "date                                   ...                            \n",
              "2000-01-01             19.3      22.6  ...             28.9        2.9\n",
              "2000-01-02             18.6      21.7  ...             38.9        3.6\n",
              "2000-01-03             18.7      20.8  ...             47.8        3.2\n",
              "\n",
              "[3 rows x 399 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 994
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzURUuSCLUYb"
      },
      "source": [
        "# 城市drop名單\n",
        "# city_drop = ['KLU', 'TPE', 'TPH', 'TYC', 'HSC', 'HSH', 'MAL', 'TXG', 'CWH', \n",
        "#         'NTO', 'YLH', 'CYI', 'CHY', 'TNN', 'KHH', 'IUH', 'ILN', 'HWA', 'TTT']\n",
        "city_drop_list = ['KLU', 'TPH', 'TYC', 'HSC', 'HSH', 'MAL', 'TXG', 'CYI']\n",
        "city_drop_columns = []\n",
        "for i in city_drop_list:\n",
        "    city_drop_columns.append([s for s in list(df_date.columns) if s.__contains__(i)])\n",
        "city_drop_columns = list(np.array(city_drop_columns).reshape(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IyKPYlYp02"
      },
      "source": [
        "# 資料預處理 - 颱風"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1noUmaUumFQ-"
      },
      "source": [
        "# 計算兩個日期間隔多少天\n",
        "def daysBetweenDate(startdate: str, enddate: str) -> int:\n",
        "    startdate = datetime.strptime(startdate, \"%Y-%m-%d\")\n",
        "    enddate = datetime.strptime(enddate, \"%Y-%m-%d\")\n",
        "    days = (enddate - startdate).days + 1\n",
        "    return days\n",
        "\n",
        "# 日期調整\n",
        "def dateShift(startdate: str, shiftday: int) -> str:\n",
        "    startdate = datetime.strptime(startdate, \"%Y-%m-%d\")\n",
        "    targetdate = startdate + timedelta(days=shiftday)\n",
        "    return datetime.strftime(targetdate, \"%Y-%m-%d\")\n",
        "\n",
        "# 讀取颱風資料庫\n",
        "df_typhoon = pd.read_csv('typhoon.csv', encoding='utf-8')\n",
        "\n",
        "# 將Warning的日期文字轉為4個欄位'startdate','starttime','enddate','endtime'\n",
        "df_typhoon[['startdate','starttime','enddate','endtime']] = df_typhoon['Warning'].str.split().tolist()\n",
        "# 將最前面塞入date欄位\n",
        "df_typhoon_new = pd.DataFrame(columns=df_typhoon.columns.insert(0, 'date'))\n",
        "\n",
        "# 將所有颱風按日期列出\n",
        "# 使用iterrows\n",
        "start_time = time.time()\n",
        "for index, row in df_typhoon.iterrows():\n",
        "    days = daysBetweenDate(row['startdate'], row['enddate'])\n",
        "    for day in range(0, days):\n",
        "        date = dateShift(row['startdate'],day)\n",
        "        datesr1 = pd.Series(date).append(df_typhoon.iloc[index]).rename({0: 'date'})\n",
        "        df_typhoon_new = df_typhoon_new.append(datesr1, ignore_index=True)\n",
        "\n",
        "# 將相同日期的去除並暫時只留WarnMark欄位\n",
        "df_typhoon_wm = pd.DataFrame(df_typhoon_new, columns=['date'])\n",
        "df_typhoon_wm['WarnMark'] = 1\n",
        "df_typhoon_wm = df_typhoon_wm.drop_duplicates().reset_index().drop(columns=['index'])\n",
        "\n",
        "df_typhoon = df_typhoon_wm.set_index('date')\n",
        "\n",
        "print(df_typhoon.head(3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAjqMc-BbUey"
      },
      "source": [
        "# 資料預處理 - 市場\n",
        "1. 選定 台北一 市場\n",
        "2. 補空值"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPTZNEJlOzoD"
      },
      "source": [
        "# 要移除的欄位列表\n",
        "# market columns 全部列表: 'Date', 'Market', 'Product', 'Up_price', 'Mid_price', 'Low_price', 'Avg_price', 'Volume', 'Month', 'Week_day', 'Year', 'Rest_day'\n",
        "market_drop_columns = [\n",
        "              'Product',\n",
        "              # 'Month', \n",
        "              # 'Week_day', \n",
        "              'Year', \n",
        "              'Rest_day'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnx4Z_ETPBzy"
      },
      "source": [
        "# 讀取農產品資料\n",
        "# if not os.path.exists('banana.csv'): urllib.request.urlretrieve(price_data_url, 'banana.csv')\n",
        "\n",
        "df = pd.read_csv(dataset_path + crop_dict[crop_no][1] + '.csv', encoding='utf-8')\n",
        "\n",
        "# csv_name = crop_dict[crop_no][0] + '.csv'\n",
        "# df = pd.read_csv(csv_name, encoding='utf-8')\n",
        "# 移除不需要的欄位\n",
        "df = df.drop(market_drop_columns, axis=1)\n",
        "print(df.head(3))\n",
        "print(df.tail(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT7-xcIZQxsV"
      },
      "source": [
        "# 將休市價格填入前後日之平均價格(暫不使用)\n",
        "def fillna_fb_mean(self):\n",
        "    df_f = self.fillna(method='ffill')\n",
        "    df_b = self.fillna(method='bfill')\n",
        "    df_fb = (df_f+df_b)/2\n",
        "    return df_fb\n",
        "\n",
        "# price_na_del = False\n",
        "# 去除價格空值者\n",
        "if price_na_del:\n",
        "    df = df\n",
        "else:\n",
        "    # 將休市價格填入前一日價格\n",
        "    df = df.fillna(method=\"ffill\")\n",
        "    \n",
        "# 只拿出指定市場的資料\n",
        "df = df[df.Market == market_dict[market_no]]\n",
        "# 去除空值\n",
        "df = df.dropna()\n",
        "df_crop = df.reset_index().drop(['index'], axis=1)\n",
        "print(df_crop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSr56sBzB2Hd"
      },
      "source": [
        "# data_dum = pd.get_dummies(data)\n",
        "# pd.DataFrame(data_dum)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "onehotencoder = OneHotEncoder()\n",
        "df_crop_month_ohe = onehotencoder.fit_transform(df_crop[[\"Month\"]]).toarray()\n",
        "month = pd.DataFrame(df_crop_month_ohe)\n",
        "for i in month:\n",
        "    new = int(i) + 1\n",
        "    new = str(new)\n",
        "    month = month.rename(columns={i:new})\n",
        "month = month.add_prefix(\"Month_\")\n",
        "# df.loc[df['city'] == '基隆市'].add_suffix('_' + 'KLU')\n",
        "# df.rename(columns={'舊欄位名稱': '新欄位名稱'}) .add_prefix(\"Month_\")\n",
        "onehotencoder = OneHotEncoder()\n",
        "df_crop_week_ohe = onehotencoder.fit_transform(df_crop[[\"Week_day\"]]).toarray()\n",
        "week = pd.DataFrame(df_crop_week_ohe)\n",
        "for i in week:\n",
        "    new = int(i) + 1\n",
        "    new = str(new)\n",
        "    week = week.rename(columns={i:new})\n",
        "week = week.add_prefix(\"Week_day_\")\n",
        "\n",
        "df_crop = df_crop.join(month, how=\"left\")\n",
        "df_crop = df_crop.join(week, how=\"left\")\n",
        "df_crop = df_crop.drop(['Month', 'Week_day'], axis=1).rename(columns={'Date': 'date'}).set_index('date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2Deb69DoKrN"
      },
      "source": [
        "df_crop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yxk48tt7xMu"
      },
      "source": [
        "# 資料預處理 - 氣象(天氣與颱風)\n",
        "1. 決定 市場 是否將合併 天氣與颱風\n",
        "2. 選出 與模型無關者排除\n",
        "3. 確認無空值\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWbmtqLZSPFv"
      },
      "source": [
        "df_all 資料合併之參數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLk3J9RWk2BH"
      },
      "source": [
        "df_all = df_crop\n",
        "# 是否要合併天氣資料\n",
        "if add_weather_data:\n",
        "    df_all = pd.merge(df_all, df_weather, how='inner', left_index = True, right_index = True)\n",
        "# 是否要合併颱風資料\n",
        "if add_typhoon_data:\n",
        "    df_all = pd.merge(df_all, df_typhoon, how='left', left_index = True, right_index = True).fillna(0)\n",
        "\n",
        "# 把平均價格移到最後1欄\n",
        "col_Avg_price = df_all.pop('Avg_price')\n",
        "df_all = pd.concat([df_all, col_Avg_price], 1)\n",
        "print(df_all.head(3))\n",
        "print(df_all.tail(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhwnpQmc5m22"
      },
      "source": [
        "Dataset 空值數量確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEch_sb6cZt0"
      },
      "source": [
        "# 確認無空值\n",
        "def dataset_na_check():\n",
        "    market_tp1_na_count = df_crop.isna().sum()\n",
        "    weather_na_count = df_weather.isna().sum()\n",
        "\n",
        "    total_na_count = list() # 氣象空值查找\n",
        "    for i in range(len(weather_na_count.index)):\n",
        "        if weather_na_count.values[i] != 0 :\n",
        "            total_na_count.append([weather_na_count.index[i], weather_na_count.values[i]])\n",
        "\n",
        "    print(market_tp1_na_count)\n",
        "    print('--------------------')\n",
        "    print(weather_na_count)\n",
        "    print('--------------------')\n",
        "    print(len(total_na_count))\n",
        "dataset_na_check()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_ui0i26N-Ik"
      },
      "source": [
        "資料Batch化函式 \\\n",
        "1. buildX\n",
        "2. buildY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBsA-1-2QyLD"
      },
      "source": [
        "# 模型輸入值建立"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-1rOEm5caxn"
      },
      "source": [
        "# 將資料整理為x\n",
        "def buildX(train, pastDay=30, futureDay=5):\n",
        "    x = []\n",
        "    for i in range(train.shape[0] - futureDay - pastDay):\n",
        "        x.append(train[i : i+pastDay])\n",
        "    return np.array(x)\n",
        "\n",
        "# 將資料整理為y\n",
        "def buildY(test, pastDay=30, futureDay=5):\n",
        "    y = []\n",
        "    for i in range(test.shape[0] - futureDay - pastDay):\n",
        "        y.append(test[i+pastDay+futureDay : i+pastDay+futureDay+1, -1])\n",
        "    return np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G26JtTk_cKEj"
      },
      "source": [
        "# pdy = 30\n",
        "# fdy = 5\n",
        "# x = []\n",
        "# y = []\n",
        "# for i in range(40 - fdy - pdy):\n",
        "#     x.append(i+pdy-1)\n",
        "# for i in range(40 - fdy - pdy):\n",
        "#     y.append(i + pdy + fdy)\n",
        "# x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy8pXTxJfEAD"
      },
      "source": [
        "依照訓練、測試的期間來切分資料 \\\n",
        "df_train 與 df_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbPs30Ly7XjS"
      },
      "source": [
        "# 將資料複製一份來作業, 將欄位index改為date\n",
        "df = df_all.copy()\n",
        "df = df.reset_index().rename(columns={'index': 'date'})\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT8V3-ZAA6kv"
      },
      "source": [
        "\n",
        "# 依訓練資料的期間、測試資料的期間來切分資料\n",
        "df_train = df.iloc[df[(train_start_date <= df.date) & (df.date <= train_end_date)].index].set_index('date')\n",
        "df_test = df.iloc[df[(test_start_date <= df.date) & (df.date <= test_end_date)].index].set_index('date')\n",
        "\n",
        "# 將非數字的欄位移除\n",
        "df_train = df_train.select_dtypes(exclude=['object'])\n",
        "df_test = df_test.select_dtypes(exclude=['object'])\n",
        "\n",
        "print(df_train.shape)\n",
        "print(df_test.shape)\n",
        "print(df_train.dtypes)\n",
        "\n",
        "# print(df_train.isna().sum().sum())\n",
        "# print(df_test.isna().sum().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZPYArmwFbZc"
      },
      "source": [
        "x_train, x_test 為  MinMaxScaler後之資料 \\\\\n",
        "y_train, y_test 為  MinMaxScaler後之價格資料\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkOqPtSkSaGx"
      },
      "source": [
        "gg = df_test.values\n",
        "X = []\n",
        "for i in range(len(df_test)):\n",
        "\tX.append(gg[i])\n",
        "X = np.array(X)\n",
        "X, df_test.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VHQWbakBRk5"
      },
      "source": [
        "# train 正則化\n",
        "df_train_scaled = df_train.values\n",
        "xx_scale = MinMaxScaler()\n",
        "x_train = buildX(xx_scale.fit_transform(df_train_scaled), pastDay, futureDay)\n",
        "print('x_train.shape', x_train.shape)\n",
        "\n",
        "yy_scale = MinMaxScaler()\n",
        "y_train_fitted_data = yy_scale.fit_transform(df_train_scaled[:, -1].reshape(-1, 1))\n",
        "y_train = buildY(y_train_fitted_data, pastDay, futureDay)\n",
        "print('y_train.shape: ', y_train.shape)\n",
        "\n",
        "# test 正則化\n",
        "df_test_scaled = df_test.values\n",
        "x_test = buildX(xx_scale.fit_transform(df_test_scaled), pastDay, futureDay)\n",
        "print('x_test.shape: ', x_test.shape)\n",
        "\n",
        "y_test = buildY(yy_scale.fit_transform(df_test_scaled[:, -1].reshape(-1, 1)), pastDay, futureDay)\n",
        "print('y_test.shape: ', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xGVUasIZjjT"
      },
      "source": [
        "# 模型選擇"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SALiD48rntdc"
      },
      "source": [
        "# 模型1 (純雙層LSTM)\n",
        "def buildManyToOneModel01(shape):\n",
        "    model = Sequential()\n",
        "    # model.add(GRU(units=256,\n",
        "    #     return_sequences=False,\n",
        "    #     input_shape=(shape[1], shape[2])))\n",
        "    \n",
        "    model.add(LSTM(units=LSTM_unit_1,\n",
        "        return_sequences=True,\n",
        "        input_shape=(shape[1], shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=LSTM_unit_2, return_sequences=False,))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugjSTprZBaYL"
      },
      "source": [
        "# 模型2 (LSTM 搭配Biderectional)\n",
        "def buildManyToOneModel02(shape):\n",
        "    model = Sequential()\n",
        "    # model.add(GRU(units=256,\n",
        "    #     return_sequences=False,\n",
        "    #     input_shape=(shape[1], shape[2])))\n",
        "    \n",
        "    # model.add(Bidirectional(LSTM(units=LSTM_unit_1,\n",
        "    #     return_sequences=True,\n",
        "    #     input_shape=(shape[1], shape[2])\n",
        "    #     )))\n",
        "    model.add(LSTM(units=LSTM_unit_1,\n",
        "        return_sequences=True,\n",
        "        input_shape=(shape[1], shape[2])\n",
        "        ))\n",
        "    model.add(Dropout(0.2))\n",
        "    # model.add(LSTM(units=LSTM_unit_2, \n",
        "    #             return_sequences=False, \n",
        "    #             input_shape=(shape[1], shape[2]),\n",
        "    #             go_backwards=True\n",
        "    #             ))\n",
        "    model.add(Bidirectional(LSTM(units=LSTM_unit_2, \n",
        "                return_sequences=False, \n",
        "                input_shape=(shape[1], shape[2]),\n",
        "                go_backwards=True\n",
        "                )))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    model.build((None, shape[1], shape[2]))\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baTga_umahTw"
      },
      "source": [
        "# 模型3\n",
        "def buildManyToOneModel03(shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(LSTM_unit_1, return_sequences=False, input_shape=(shape[1], shape[2])))\n",
        "    # model.add(Dropout(0.2))\n",
        "    # model.add(LSTM(LSTM_unit_2, return_sequences=True))\n",
        "    # model.add(Dropout(0.2))\n",
        "    # model.add(LSTM(LSTM_unit_3, return_sequences=True))\n",
        "    # model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # model.add(TimeDistributed(Dense(1)))\n",
        "    # layers = [\n",
        "    #     LSTM(units=LSTM_unit_1, return_sequences=False, input_shape=(shape[1], shape[2])),\n",
        "    #     # Dense(units=128, activation=\"relu\"),\n",
        "    #     # Dense(units=10, activation=\"relu\"),\n",
        "    #     Dense(units=1, activation='sigmoid')]\n",
        "    # model = Sequential(layers)\n",
        "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mse\"])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvVsnT3kQA1u"
      },
      "source": [
        "# 模型訓練"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIIZHwhmOslY"
      },
      "source": [
        "# 模型字典: function name, 說明, 是否shift day, 是否只輸出1天\n",
        "model_dict = {\n",
        "    1 : [buildManyToOneModel01, 'LSTM many to one', True, True],\n",
        "    2 : [buildManyToOneModel02, 'LSTM & Biderectional many to one', True, True],\n",
        "    3 : [buildManyToOneModel03, 'One LSTM to one', True, True],\n",
        "    # 4 : [buildManyToOneModel4, 'LSTM many to one', True, True],\n",
        "    5 : ['', ''],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPrrCMXtaJoy"
      },
      "source": [
        "# 模型訓練\n",
        "if model_no == 1:\n",
        "    model = buildManyToOneModel01(x_train.shape)\n",
        "    print('model1')\n",
        "    model_name = 'buildManyToOneModel01'\n",
        "elif model_no == 2:\n",
        "    model = buildManyToOneModel02(x_train.shape)\n",
        "    print('model2')\n",
        "    model_name = 'buildManyToOneModel02'\n",
        "elif model_no == 3:\n",
        "    model = buildManyToOneModel03(x_train.shape)\n",
        "    model_name = 'buildManyToOneModel03'\n",
        "    if repeat_train:\n",
        "        model_repeat1 = clone_model(model)\n",
        "        model_repeat2 = clone_model(model)\n",
        "        model_repeat3 = clone_model(model)\n",
        "        model_repeat4 = clone_model(model)\n",
        "        model_repeat5 = clone_model(model)\n",
        "    print('model3')\n",
        "else:\n",
        "    print('選錯model了')\n",
        "\n",
        "callback = EarlyStopping(monitor=\"val_loss\", patience=patience, verbose=1, mode=\"auto\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g0ZeHa5Jenm"
      },
      "source": [
        "# 模型相關函式建立"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9NHw0Pu9hSj"
      },
      "source": [
        "def model_load(h5_name):\n",
        "    model = load_model(f'{save_google_dir}h5/{h5_name}')\n",
        "    print(\"MODEL-LOADED\")\n",
        "    return model\n",
        "# def model_fit():\n",
        "#     model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=[callback])\n",
        "#     return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XjF7XnatNLt"
      },
      "source": [
        "# scaler 存檔\n",
        "def scaler_save():\n",
        "    # joblib.dump(xx_scale, save_google_dir + 'scaler/' + f'{crop_dict[crop_no][0]}_{time_now}_P{pastDay}F{futureDay}_RMSE={int(round(RMSE, 0))}' + 'X_scaler.model')\n",
        "    # joblib.dump(yy_scale, save_google_dir + 'scaler/' + f'{crop_dict[crop_no][0]}_{time_now}_P{pastDay}F{futureDay}_RMSE={int(round(RMSE, 0))}' + 'Y_scaler.model')\n",
        "    joblib.dump(xx_scale, save_google_dir + 'scaler/' + f'{time_now}_{crop_no}-{crop_dict[crop_no][0]}_D{futureDay}_M23_' + 'X_scaler.model')\n",
        "    joblib.dump(yy_scale, save_google_dir + 'scaler/' + f'{time_now}_{crop_no}-{crop_dict[crop_no][0]}_D{futureDay}_M23_' + 'Y_scaler.model')\n",
        "# scaler模型獲取\n",
        "def scaler_load(scaler_time):\n",
        "    # xx_scale = joblib.load(save_google_dir + 'scaler/' + f'{crop_dict[crop_no][0]}_{scaler_time}_P{pastDay}F{futureDay}_RMSE={int(round(RMSE, 0))}' + 'X_scaler.model')\n",
        "    # yy_scale = joblib.load(save_google_dir + 'scaler/' + f'{crop_dict[crop_no][0]}_{scaler_time}_P{pastDay}F{futureDay}_RMSE={int(round(RMSE, 0))}' + 'Y_scaler.model')\n",
        "    xx_scale = joblib.load(save_google_dir + 'scaler/' + f'{crop_no}-{crop_dict[crop_no][0]}_D{futureDay}_M23_' + 'X_scaler.model')\n",
        "    yy_scale = joblib.load(save_google_dir + 'scaler/' + f'{crop_no}-{crop_dict[crop_no][0]}_D{futureDay}_M23_' + 'Y_scaler.model')\n",
        "    return xx_scale, yy_scale\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB3evn6deN0M"
      },
      "source": [
        "def model_save(model):\n",
        "    # 儲存模型\n",
        "    # if RMSE_dict[RMSE_min][2] == 6:\n",
        "    # if RMSE_dict[RMSE_esb][2] == 6:\n",
        "    #     print('123')\n",
        "    #     for model in RMSE_dict[RMSE_min][1]:\n",
        "    #         model.save((time_now) + f'_{}' + '.h5')\n",
        "    #         if save_google: model.save(save_google_dir + 'h5/' + (time_now) + '.h5')   \n",
        "    # else:\n",
        "        # model.save((time_now) + '.h5')\n",
        "        # if save_google: model.save(save_google_dir + 'h5/' + (time_now) + '.h5')\n",
        "    model.save((time_now) + '.h5')\n",
        "    # if save_google: model.save(save_google_dir + 'h5/' + f'{crop_dict[crop_no][0]}_{time_now}_P{pastDay}F{futureDay}_RMSE={int(round(RMSE, 0))}.h5')\n",
        "    if save_google: model.save(save_google_dir + 'h5/' + f'{time_now}_{crop_no}-{crop_dict[crop_no][0]}_D{futureDay}_M23.h5')\n",
        "    print('MODEL-SAVED')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvlf3x2A2tJK"
      },
      "source": [
        "fit()用於訓練具有給定輸入的模型。\n",
        "\n",
        "predict()用於實際預測。它爲輸入樣本生成輸出預測。\n",
        "\n",
        "evaluate()用於評估已經過訓練的模型。返回模型的損失值&指標值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vm1ifAdy8ZL"
      },
      "source": [
        "def model_cal(model):\n",
        "    score = model.evaluate(x_test, y_test)\n",
        "    print('Score: {}'.format(score))\n",
        "    y_pre = model.predict(x_test)\n",
        "    print('y_pre.shape:', y_pre.shape, 'y_test.shape:', y_test.shape)\n",
        "    return y_pre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m55YI86mQKFr"
      },
      "source": [
        "# 模型產出、處理、作圖與儲存"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiAkI9_x18cP"
      },
      "source": [
        "預測結果與實際結果的 數值迴轉 \\\n",
        "y_pre  --> pre_price \\\n",
        "y_test --> test_price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHTtevSlTkwr"
      },
      "source": [
        "def price_inverse(y_pre):\n",
        "    pre_price = yy_scale.inverse_transform(y_pre)\n",
        "    test_price = yy_scale.inverse_transform(y_test)\n",
        "    diff_price = pre_price - test_price\n",
        "    # print('pre_price.shape: ', pre_price.shape)\n",
        "    # print('test_price.shape: ', test_price.shape)\n",
        "    # print('diff_price.shape: ', diff_price.shape)\n",
        "    # print('pre_price[:3]: ', pre_price[:3])\n",
        "    return pre_price, test_price, diff_price"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlGeWbIN8tb6"
      },
      "source": [
        "# 畫圖\n",
        "def DrawingPlot(pre_price, test_price, pic_days):\n",
        "    plt.figure(figsize=(15,5))\n",
        "    plt.plot(test_price[-pic_days:], label='Real Price')\n",
        "    plt.plot(pre_price[-pic_days:], label='Predict Price')\n",
        "    # plt.bar(np.arange(pic_days), diff_price[-pic_days:].reshape(-1), color='r', label='Diff', align='center')\n",
        "    # plt.plot(diff_price[-pic_days:], color='r', label='Diff')\n",
        "    plt.xlabel('day')   \n",
        "    plt.ylabel('price')\n",
        "    plt.title(crop_dict[crop_no][0] + ' predict D' + str(futureDay) + ' price')\n",
        "    # plt.title(f'model-no.{model_no}_{model_version}_{pastDay}days for D{futureDay}')\n",
        "    plt.legend()\n",
        "    if save_google: plt.savefig(save_google_dir + 'img/' + f'{crop_dict[crop_no][0]}_{time_now}_P{pastDay}F{futureDay}_RMSE={int(round(RMSE, 0))}' + '.png')\n",
        "    # if save_google: plt.savefig(save_google_dir + 'img/' + time_now + '_D' + str(futureDay) + '.png')\n",
        "    # plt.savefig(f'{save_google_dir}img/{model_version}_{time_now}.png')  \n",
        "    print('pic_saved')\n",
        "    plt.show()\n",
        "# f'{crop_dict[crop_no][0]}_{time_now}_P{pastDay}F{futureDay}_RMSE={int(round(RMSE, 0))}.h5'\n",
        "# f'{crop_dict[crop_no][0]}_{time_now}_P{pastDay}F{futureDay}_RMSE={int(round(RMSE, 0))}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80Z8RzTPwO42"
      },
      "source": [
        "# 計分\n",
        "def score_cal(pre_price):\n",
        "    MSE = mean_squared_error(test_price.reshape(-1, 1), pre_price.reshape(-1, 1))\n",
        "    RMSE = np.sqrt(MSE)\n",
        "    MAE = mean_absolute_error(test_price.reshape(-1, 1), pre_price.reshape(-1, 1))\n",
        "    R2 = r2_score(test_price.reshape(-1, 1), pre_price.reshape(-1, 1))\n",
        "    print(f\"MSE value : {MSE}\", f\"\\nRMSE value : {RMSE}\", f\"\\nMAE value : {MAE}\", f\"\\nR2 score value : {R2}\")\n",
        "    return MSE, RMSE, MAE, R2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md9K67WAWOxi"
      },
      "source": [
        "儲存每一次試驗資訊"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZIWH9w7ZAAz"
      },
      "source": [
        "# 記錄結果\n",
        "result_column_lists = ['time_now', 'crop_name', 'market_name', 'add_weather_data', 'add_typhoon_data', 'train_start_date', \n",
        "              'x_train.shape', 'x_test.shape', \n",
        "              'model_no', 'model_name', 'pastday', 'futureDay', 'batch_size', 'epochs', 'validation_split', 'patience', \n",
        "              'predDay', 'MSE', 'RMSE', 'MAE', 'R2', 'weather_drop_columns', 'city_drop_list', 'market_drop_columns', 'dev_notes', 'LSTM_unit_1', 'LSTM_unit_2', 'repeat_train', \n",
        "              'ohe', 'price_na_del']\n",
        "\n",
        "# def saveResult(path, preDay, real_value, pred_value):\n",
        "#   with open(path, 'a', newline='', encoding='utf-8') as f:\n",
        "#     result_writer = csv.writer(f)\n",
        "#     if f.tell()==0: result_writer.writerow(result_column_lists)\n",
        "#     for i in range(preDay):\n",
        "#       # i + 1 = 未來1日的價格\n",
        "#       Dday = i + 1\n",
        "#       # real_price = real_value[Dday:]\n",
        "#       # pred_price = pred_value[:-Dday,i]\n",
        "\n",
        "#       # MSE = mean_squared_error(real_price, pred_price)\n",
        "#       # RMSE = np.sqrt(MSE)\n",
        "#       # R2 = r2_score(real_price, pred_price)\n",
        "      \n",
        "#       result_lists = [datetime_now, crop_dict[crop_no][1], market_dict[market_no], add_weather_data, add_typhoon_data, train_start_date, model_no, model_dict[model_no][1], pastDay, futureDay, batch_size, epochs, validation_split, Dday, MSE, RMSE, MAE, R2]\n",
        "#       result_writer.writerow(result_lists)\n",
        "\n",
        "def saveResultOne(path, preDay):\n",
        "    with open(path, 'a', newline='', encoding='utf-8') as f:\n",
        "        result_writer = csv.writer(f)\n",
        "        if f.tell()==0: result_writer.writerow(result_column_lists)\n",
        "        # real_price = real_value\n",
        "        # pred_price = pred_value\n",
        "        # MSE = mean_squared_error(real_price, pred_price)\n",
        "        # RMSE = np.sqrt(MSE)\n",
        "        # R2 = r2_score(real_price, pred_price)\n",
        "    \n",
        "        result_lists = [time_now, crop_dict[crop_no][1], market_dict[market_no], add_weather_data, add_typhoon_data, train_start_date, \n",
        "                    x_train.shape, x_test.shape, \n",
        "                    model_no, model_dict[model_no][1], pastDay, futureDay, batch_size, epochs, validation_split, patience,\n",
        "                    preDay, MSE, RMSE, MAE, R2, weather_drop_columns, city_drop_list, market_drop_columns, dev_notes, LSTM_unit_1, LSTM_unit_2, f'repeat={repeat_train}', \n",
        "                    ohe, price_na_del]\n",
        "        result_writer.writerow(result_lists)      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Eb0jDPMSTuD"
      },
      "source": [
        "# 執行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvsxqbo5mSN5"
      },
      "source": [
        "if model_no == 3 and repeat_train == True:\n",
        "    model_repeat1.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mse\"])\n",
        "    train_history1 = model_repeat1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=[callback])\n",
        "    model_repeat2.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mse\"])\n",
        "    train_history2 = model_repeat2.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=[callback])\n",
        "    model_repeat3.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mse\"])\n",
        "    train_history3 = model_repeat3.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=[callback])\n",
        "    model_repeat4.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mse\"])\n",
        "    train_history4 = model_repeat4.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=[callback])\n",
        "    model_repeat5.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mse\"])\n",
        "    train_history5 = model_repeat5.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=[callback])\n",
        "else:\n",
        "    train_history1 =  model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=[callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt-K5Mu8XvKk"
      },
      "source": [
        "# # 模型3-2\n",
        "# model_repeat2 = Sequential()\n",
        "# model_repeat2.add(LSTM(LSTM_unit_1, return_sequences=False, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
        "# model_repeat2.add(Dense(1, activation='sigmoid'))\n",
        "# model_repeat2.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "# model_repeat2.summary()\n",
        "# train_history2 =  model_repeat2.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=[callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28pQrxPSbFSS"
      },
      "source": [
        "# # 模型3-3\n",
        "# model_repeat3 = Sequential()\n",
        "# model_repeat3.add(LSTM(LSTM_unit_1, return_sequences=False, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
        "# model_repeat3.add(Dense(1, activation='sigmoid'))\n",
        "# model_repeat3.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "# model_repeat3.summary()\n",
        "# train_history3 =  model_repeat3.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=[callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GekiRji31D8U"
      },
      "source": [
        "# # 模型3-4\n",
        "# model_repeat4 = Sequential()\n",
        "# model_repeat4.add(LSTM(LSTM_unit_1, return_sequences=False, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
        "# model_repeat4.add(Dense(1, activation='sigmoid'))\n",
        "# model_repeat4.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "# model_repeat4.summary()\n",
        "# train_history4 =  model_repeat4.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=[callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6igYq5t1EF8"
      },
      "source": [
        "# # 模型3-5\n",
        "# model_repeat5 = Sequential()\n",
        "# model_repeat5.add(LSTM(LSTM_unit_1, return_sequences=False, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
        "# model_repeat5.add(Dense(1, activation='sigmoid'))\n",
        "# model_repeat5.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "# model_repeat5.summary()\n",
        "# train_history5 =  model_repeat5.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=[callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5gJ7IeJblci"
      },
      "source": [
        "# 執行模型，預測價格\n",
        "if model_no == 3 and repeat_train == True:\n",
        "    y_pre1 = model_cal(model_repeat1)\n",
        "    y_pre2 = model_cal(model_repeat2)\n",
        "    y_pre3 = model_cal(model_repeat3)\n",
        "    y_pre4 = model_cal(model_repeat4)\n",
        "    y_pre5 = model_cal(model_repeat5)\n",
        "else:\n",
        "    y_pre = model_cal(model) \n",
        "  \n",
        "# MSE, RMSE, MAE, R2 = score_cal()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L-I7ZRh2pQX"
      },
      "source": [
        "# 計算分數\n",
        "if model_no == 3 and repeat_train == True:\n",
        "    # 建立各模型的轉換價格，並計算分數\n",
        "    pre_price1, test_price, diff_price1 = price_inverse(y_pre1)\n",
        "    pre_price2, test_price, diff_price2 = price_inverse(y_pre2)\n",
        "    pre_price3, test_price, diff_price3 = price_inverse(y_pre3)\n",
        "    pre_price4, test_price, diff_price4 = price_inverse(y_pre4)\n",
        "    pre_price5, test_price, diff_price5 = price_inverse(y_pre5)\n",
        "    print('*'*10 + '1' +'*'*10)\n",
        "    MSE_1, RMSE_1, MAE_1, R2_1 = score_cal(pre_price1)\n",
        "    print('*'*10 + '2' +'*'*10)\n",
        "    MSE_2, RMSE_2, MAE_2, R2_2 = score_cal(pre_price2)\n",
        "    print('*'*10 + '3' +'*'*10)\n",
        "    MSE_3, RMSE_3, MAE_3, R2_3 = score_cal(pre_price3)\n",
        "    print('*'*10 + '4' +'*'*10)\n",
        "    MSE_4, RMSE_4, MAE_4, R2_4 = score_cal(pre_price4)\n",
        "    print('*'*10 + '5' +'*'*10)\n",
        "    MSE_5, RMSE_5, MAE_5, R2_5 = score_cal(pre_price5)\n",
        "    pre_price_list = [pre_price1, pre_price2, pre_price3, pre_price4, pre_price5 ]\n",
        "    # # ensemble轉換價格，並計算分數\n",
        "    # ensemble_pre_price = (pre_price1 + pre_price2 + pre_price3 + pre_price4 + pre_price5)/5\n",
        "    # MSE_esb, RMSE_esb, MAE_esb, R2_esb = score_cal(ensemble_pre_price)\n",
        "\n",
        "    # 建立RMSE字典， 並由之找出最小值作為最終模型\n",
        "    # RMSE_min = min([RMSE_1, RMSE_2, RMSE_3, RMSE_4, RMSE_5, RMSE_esb])\n",
        "    RMSE_min = min([RMSE_1, RMSE_2, RMSE_3, RMSE_4, RMSE_5])\n",
        "    RMSE_dict = {\n",
        "        RMSE_1:[score_cal(pre_price1), model_repeat1, 1], \n",
        "        RMSE_2:[score_cal(pre_price2), model_repeat2, 2], \n",
        "        RMSE_3:[score_cal(pre_price3), model_repeat3, 3], \n",
        "        RMSE_4:[score_cal(pre_price4), model_repeat4, 4],\n",
        "        RMSE_5:[score_cal(pre_price5), model_repeat5, 5],\n",
        "        # RMSE_esb:[score_cal(ensemble_pre_price), [model_repeat1, \n",
        "        #                        model_repeat2,\n",
        "        #                        model_repeat3,\n",
        "        #                        model_repeat4,\n",
        "        #                        model_repeat5], 6], \n",
        "    }\n",
        "\n",
        "    # 確認1-5中最終版之MSE, RMSE, MAE, R2, pre_price\n",
        "    MSE, RMSE, MAE, R2 = RMSE_dict[RMSE_min][0]\n",
        "    pre_price = pre_price_list[RMSE_dict[RMSE_min][2]-1]\n",
        "else:\n",
        "    # 轉換價格，並計算分數\n",
        "    pre_price, test_price, diff_price = price_inverse(y_pre)\n",
        "    MSE, RMSE, MAE, R2 = score_cal(pre_price)\n",
        "\n",
        "print(MSE, RMSE, MAE, R2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXjzi3QwJC1l"
      },
      "source": [
        "# 新增區段"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMNQ3QusTV6g"
      },
      "source": [
        "# h5_list = ['banana_prediction_model_v2_s2_0728-13:15.h5']\n",
        "# for h5_name in h5_list: \n",
        "#     model_load(h5_name) \n",
        "\n",
        "# y_pre = model_cal()\n",
        "# pre_price, test_price, diff_price = price_inverse()\n",
        "# MSE, RMSE, MAE, R2 = score_cal()\n",
        "# clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgMqVHWYmVhU"
      },
      "source": [
        "# 統計數值建立\n",
        "asd = list(df['Avg_price'].values)\n",
        "print(np.mean(asd))\n",
        "print(np.median(asd))\n",
        "print(np.percentile(asd, [25, 50, 75]))\n",
        "print(MAE/np.mean(asd))\n",
        "MAE/np.percentile(asd, [25, 50, 75])\n",
        "np.std(asd, ddof=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQt-bqKV7f2O"
      },
      "source": [
        "# 印出第一個模型\n",
        "plt.title('Loss Graph')\n",
        "plt.plot(train_history1.history['loss'], 'blue', label='loss')\n",
        "plt.plot(train_history1.history['val_loss'], 'red', label='Validation loss')\n",
        "plt.legend(loc=\"upper left\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w7SY8mcUPHA"
      },
      "source": [
        "# save the result\n",
        "is_output_one = model_dict[model_no][3]\n",
        "if is_output_one:\n",
        "    saveResultOne('result.csv', futureDay)\n",
        "    if save_google: saveResultOne(save_google_dir + 'result.csv', futureDay)\n",
        "# else: \n",
        "#     saveResult('result.csv', futureDay, rp, pp)\n",
        "#     if save_google: saveResult(save_google_dir + 'result.csv', futureDay, rp, pp)\n",
        "\n",
        "if model_no ==3:\n",
        "    model_save(RMSE_dict[RMSE_min][1])\n",
        "else:\n",
        "    model_save(model)\n",
        "scaler_save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VugvB-veSwJL"
      },
      "source": [
        "DrawingPlot(pre_price, test_price, pic_days)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJIdP0WcSPdp"
      },
      "source": [
        "https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/ \\\\\n",
        "The input to every LSTM layer must be three-dimensional.\n",
        "\n",
        "The three dimensions of this input are:\n",
        "\n",
        "1. Samples. One sequence is one sample. A batch is comprised of one or more samples.\n",
        "2. Time Steps. One time step is one point of observation in the sample.\n",
        "3. Features. One feature is one observation at a time step."
      ]
    }
  ]
}